{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "870a6406",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7960\\4027242128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;31m# Llamar a la función de entrenamiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;31m# Llamar a la función de predicción\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7960\\4027242128.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Calcular las métricas de entrenamiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mspecificity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0msensitivity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    # Crear el pipeline de ingeniería de características y modelo\n",
    "    pipeline = Pipeline([\n",
    "        # Agrega aquí tus etapas de preprocesamiento y transformación de características\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('random_forest', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    # Iniciar contador de tiempo\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Calcular el tiempo de entrenamiento en segundos\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Obtener las predicciones en los datos de entrenamiento\n",
    "    y_pred = pipeline.predict(X_train)\n",
    "\n",
    "    # Calcular las métricas de entrenamiento\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    roc_auc = roc_auc_score(y_train, y_pred)\n",
    "\n",
    "    # Generar el archivo de salida con los resultados\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_file = f\"train_results_{timestamp}.txt\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"Fecha y hora de ejecución: {}\\n\".format(timestamp))\n",
    "        file.write(\"Tiempo de entrenamiento: {:.2f} segundos\\n\".format(training_time))\n",
    "        file.write(\"Accuracy: {:.4f}\\n\".format(accuracy))\n",
    "        file.write(\"Specificity: {:.4f}\\n\".format(specificity))\n",
    "        file.write(\"Sensitivity: {:.4f}\\n\".format(sensitivity))\n",
    "        file.write(\"ROC-AUC: {:.4f}\\n\".format(roc_auc))\n",
    "\n",
    "    print(\"Entrenamiento completado. Resultados guardados en {}\".format(output_file))\n",
    "\n",
    "\n",
    "def predict(X_test):\n",
    "    # Cargar el modelo entrenado\n",
    "    pipeline = Pipeline([\n",
    "        # Agrega aquí tus etapas de preprocesamiento y transformación de características\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('random_forest', RandomForestClassifier())\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar las predicciones en los datos de prueba\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Generar el archivo CSV con las predicciones\n",
    "    predictions_df = pd.DataFrame(y_pred, columns=[\"Prediction\"])\n",
    "    predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "    print(\"Predicciones generadas. Resultados guardados en predictions.csv\")\n",
    "\n",
    "\n",
    "# Cargar los datos de entrenamiento y prueba\n",
    "dataTrain = pd.read_excel(\"DB4.xlsx\")\n",
    "cols_con_na = [col for col in dataTrain.columns if (dataTrain[col].isnull().mean() < 0.05)]\n",
    "nuevo_df = dataTrain.loc[:, cols_con_na]\n",
    "data_numericas = nuevo_df.select_dtypes(include=['float', 'int'])\n",
    "df = nuevo_df\n",
    "\n",
    "def getDateColTypes(df):\n",
    "    categoricas = []\n",
    "    continuas = []\n",
    "    discretas = []\n",
    "\n",
    "    for colName in df.columns:\n",
    "        if df[colName].dtype == \"O\":\n",
    "            categoricas.append(colName)\n",
    "        else:\n",
    "            if((df[colName].dtype == \"int64\" or df[colName].dtypes == 'float64')):\n",
    "                if (len(df[colName]) <=30):\n",
    "                    discretas.append(colName)\n",
    "                else:\n",
    "                    continuas.append(colName)\n",
    "\n",
    "    return discretas, continuas, categoricas\n",
    "\n",
    "discretas, continuas, categoricas = getDateColTypes(df)\n",
    "categoricas_df = dataTrain.loc[:, categoricas]\n",
    "continuas_df = dataTrain.loc[:, continuas]\n",
    "\n",
    "def imputar_media(datos, columna):\n",
    "    media = datos[columna].mean()\n",
    "    datos_imputados = datos.copy()\n",
    "    datos_imputados[columna].fillna(media, inplace=True)\n",
    "    return datos_imputados\n",
    "\n",
    "for columna in continuas:\n",
    "    datos_imputados_media = imputar_media(df, columna)\n",
    "\n",
    "def imputar_categorico(datos):\n",
    "    datos_imputados = categoricas_df.copy()\n",
    "    for columna in categoricas_df.columns:\n",
    "        moda = categoricas_df[columna].mode().iloc[0]\n",
    "        datos_imputados[columna].fillna(moda, inplace=True)\n",
    "    return datos_imputados\n",
    "\n",
    "df_imputado_cat = imputar_categorico(categoricas_df)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df_imputado_cat['mes_codificado'] = label_encoder.fit_transform(df_imputado_cat['mes_ocu']) + 1\n",
    "df_imputado_cat['vehiculo_codificado'] = label_encoder.fit_transform(df_imputado_cat['tipo_vehi']) + 1\n",
    "df_imputado_cat['ocu_codificado'] = label_encoder.fit_transform(df_imputado_cat['lesio_fall']) + 1\n",
    "df_imputado_cat['causa_codificado'] = label_encoder.fit_transform(df_imputado_cat['Causa_acc']) + 1\n",
    "df_imputado_cat['depto_codificado'] = label_encoder.fit_transform(df_imputado_cat['depto_ocu']) + 1\n",
    "df_imputado_cat['dia_codificado'] = label_encoder.fit_transform(df_imputado_cat['dia_sem_ocu']) + 1\n",
    "df_imputado_cat['mes_codificado'] = label_encoder.fit_transform(df_imputado_cat['mes_ocu']) + 1\n",
    "\n",
    "df_copia = df_imputado_cat.copy()\n",
    "\n",
    "columnas_eliminar = ['mes_ocu', 'dia_sem_ocu', 'depto_ocu', 'tipo_vehi', 'Causa_acc']\n",
    "\n",
    "df_copia = df_copia.drop(columnas_eliminar, axis=1)\n",
    "\n",
    "dataset = df\n",
    "\n",
    "def getColumnTypes(dataset):\n",
    "    continuas = []\n",
    "    discretas = []\n",
    "    categoricas = []\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if((dataset[col].dtype == 'int64') or (dataset[col].dtype == 'float64')):\n",
    "            if(len(dataset[col].unique())>30):\n",
    "                continuas.append(col)\n",
    "            else:\n",
    "                discretas.append(col)\n",
    "        else:\n",
    "            categoricas.append(col)\n",
    "    return continuas, discretas, categoricas\n",
    "\n",
    "df_copia2 = df_copia.drop('lesio_fall', axis=1)\n",
    "\n",
    "df3 = pd.concat([df_copia2, data_numericas], axis=1)\n",
    "df3.shape\n",
    "\n",
    "dfc2 = df3.copy()\n",
    "\n",
    "df_cleaned = dfc2.dropna()\n",
    "\n",
    "X = df_cleaned.drop(['dia_codificado', 'year', 'vehiculo_codificado', 'mes_codificado'], axis=1)\n",
    "y = df_cleaned['dia_codificado']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=125)\n",
    "\n",
    "# Llamar a la función de entrenamiento\n",
    "train_model(X_train, y_train)\n",
    "\n",
    "# Llamar a la función de predicción\n",
    "predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "48168050f1fe731b302b63a744aad32afdf0a66ee6e49905924ec3fd4edeb196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
